{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9ced0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --user -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "198209b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/swu/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/swu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import random\n",
    "import time\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# for reddit api\n",
    "from psaw import PushshiftAPI\n",
    "\n",
    "# stopword\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"vader_lexicon\")\n",
    "nltk.download('stopwords')\n",
    "sw_nltk = stopwords.words('english')\n",
    "\n",
    "import re\n",
    "text_cleaning_regex = \"@S+|https?:S+|http?:S|[^A-Za-z0-9]+\"\n",
    "\n",
    "# sklearn\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support,roc_curve, auc, f1_score,cohen_kappa_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# bert\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, WeightedRandomSampler,random_split\n",
    "from transformers import BertModel,BertForSequenceClassification, Trainer, TrainingArguments, AutoTokenizer, AdamW, get_linear_schedule_with_warmup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e19695dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "# check the device\n",
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "65845aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = PushshiftAPI()\n",
    "start_time = int(dt.datetime(2019, 3, 16).timestamp())\n",
    "end_time = int(dt.datetime(2022, 3, 10).timestamp())\n",
    "\n",
    "reddit_comments =list(api.search_comments(after = start_time\n",
    "                                  , before = end_time\n",
    "                                  , q = \"BTC price\" # search for a specific word or phrase, not case-sensitive\n",
    "#                                   , subreddit='BTC'\n",
    "                                  , filter=['body','created_utc']\n",
    "                                 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b0f0c04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(reddit_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e6b8452a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['created_at'] = pd.to_datetime(df['created_utc'],unit='s')\n",
    "df = df[[\"body\",\"created_at\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "82b29cb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>created_at</th>\n",
       "      <th>n_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes. BTC price seems HUGELY speculative to me ...</td>\n",
       "      <td>2022-03-10 02:36:20</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&amp;gt;yes, there is rampant market manipulation ...</td>\n",
       "      <td>2022-03-09 16:01:44</td>\n",
       "      <td>303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I would be happy if the price negated the infl...</td>\n",
       "      <td>2022-03-08 20:09:35</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I think we mostly agree, but this part is flat...</td>\n",
       "      <td>2022-03-07 23:58:26</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&amp;gt; That is why it forked in 2017: some of us...</td>\n",
       "      <td>2022-03-07 21:39:24</td>\n",
       "      <td>638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body          created_at  \\\n",
       "0  Yes. BTC price seems HUGELY speculative to me ... 2022-03-10 02:36:20   \n",
       "1  &gt;yes, there is rampant market manipulation ... 2022-03-09 16:01:44   \n",
       "2  I would be happy if the price negated the infl... 2022-03-08 20:09:35   \n",
       "3  I think we mostly agree, but this part is flat... 2022-03-07 23:58:26   \n",
       "4  &gt; That is why it forked in 2017: some of us... 2022-03-07 21:39:24   \n",
       "\n",
       "   n_words  \n",
       "0       66  \n",
       "1      303  \n",
       "2       39  \n",
       "3      215  \n",
       "4      638  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8881a20",
   "metadata": {},
   "source": [
    "## Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3027b4a3",
   "metadata": {},
   "source": [
    "### 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e02e9c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "    text = re.sub(text_cleaning_regex, ' ', str(text).lower()).strip()\n",
    "    res=[]\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove single letter words\n",
    "    text = ' '.join( [w for w in text.split() if len(w)>1] )\n",
    "    \n",
    "    # Remove ticks and the next character\n",
    "    text = re.sub(\"\\'\\w+\", '', text)\n",
    "    \n",
    "    # Remove stopword\n",
    "    text = ' '.join([word for word in text.split() if text.lower() not in sw_nltk])\n",
    "    # remove the word after @ OR #\n",
    "    for i in text.split():\n",
    "        if i.startswith(\"@\") or i.startswith(\"#\"):\n",
    "            continue\n",
    "        else:\n",
    "            res.append(i)\n",
    "    return ' '.join(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "003ca7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['body']=df['body'].apply(lambda x: text_preprocessing(x))\n",
    "df['n_words'] = df['body'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a19ae321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>created_at</th>\n",
       "      <th>n_words</th>\n",
       "      <th>sentiment_dict</th>\n",
       "      <th>compound</th>\n",
       "      <th>neg</th>\n",
       "      <th>pos</th>\n",
       "      <th>confidence score</th>\n",
       "      <th>vader_result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yes btc price seems hugely speculative scale v...</td>\n",
       "      <td>2022-03-10 02:36:20</td>\n",
       "      <td>40</td>\n",
       "      <td>{'neg': 0.184, 'neu': 0.638, 'pos': 0.178, 'co...</td>\n",
       "      <td>-0.3197</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.016575</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gt yes rampant market manipulation everywhere ...</td>\n",
       "      <td>2022-03-09 16:01:44</td>\n",
       "      <td>157</td>\n",
       "      <td>{'neg': 0.134, 'neu': 0.66, 'pos': 0.206, 'com...</td>\n",
       "      <td>0.9042</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.211765</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>would happy price negated inflation seeing cry...</td>\n",
       "      <td>2022-03-08 20:09:35</td>\n",
       "      <td>19</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.829, 'pos': 0.171, 'comp...</td>\n",
       "      <td>0.5719</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.171</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>think mostly agree part flat wrong gt fork rea...</td>\n",
       "      <td>2022-03-07 23:58:26</td>\n",
       "      <td>132</td>\n",
       "      <td>{'neg': 0.081, 'neu': 0.812, 'pos': 0.107, 'co...</td>\n",
       "      <td>0.5898</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.138298</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gt forked 2017 us prefer cheap reliable fast b...</td>\n",
       "      <td>2022-03-07 21:39:24</td>\n",
       "      <td>331</td>\n",
       "      <td>{'neg': 0.171, 'neu': 0.658, 'pos': 0.171, 'co...</td>\n",
       "      <td>-0.8005</td>\n",
       "      <td>-0.171</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body          created_at  \\\n",
       "0  yes btc price seems hugely speculative scale v... 2022-03-10 02:36:20   \n",
       "1  gt yes rampant market manipulation everywhere ... 2022-03-09 16:01:44   \n",
       "2  would happy price negated inflation seeing cry... 2022-03-08 20:09:35   \n",
       "3  think mostly agree part flat wrong gt fork rea... 2022-03-07 23:58:26   \n",
       "4  gt forked 2017 us prefer cheap reliable fast b... 2022-03-07 21:39:24   \n",
       "\n",
       "   n_words                                     sentiment_dict  compound  \\\n",
       "0       40  {'neg': 0.184, 'neu': 0.638, 'pos': 0.178, 'co...   -0.3197   \n",
       "1      157  {'neg': 0.134, 'neu': 0.66, 'pos': 0.206, 'com...    0.9042   \n",
       "2       19  {'neg': 0.0, 'neu': 0.829, 'pos': 0.171, 'comp...    0.5719   \n",
       "3      132  {'neg': 0.081, 'neu': 0.812, 'pos': 0.107, 'co...    0.5898   \n",
       "4      331  {'neg': 0.171, 'neu': 0.658, 'pos': 0.171, 'co...   -0.8005   \n",
       "\n",
       "     neg    pos  confidence score  vader_result  \n",
       "0 -0.184  0.178          0.016575             0  \n",
       "1 -0.134  0.206          0.211765             2  \n",
       "2 -0.000  0.171          1.000000             2  \n",
       "3 -0.081  0.107          0.138298             2  \n",
       "4 -0.171  0.171          0.000000             0  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45679bda",
   "metadata": {},
   "source": [
    "### 2. LRSentiA\n",
    "\n",
    "**training strategy** \n",
    "- Generate Labels using an Unsupervised method\n",
    "- referenece: https://openreview.net/forum?id=kQns9y_JH6, https://www.sciencedirect.com/science/article/pii/S2666827021000074\n",
    "\n",
    "\n",
    "> LRSentiA is a lexicon and rule-based method that can classify sentiment without using any labeled data. The main purpose of introducing LRSentiA is to generate accurate pseudo-labels so that supervised ML classifiers can be incorporated into SSentiA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8f3db66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "df['sentiment_dict'] = df['body'].apply(lambda x:analyzer.polarity_scores(x))\n",
    "df['compound']  = df['sentiment_dict'].apply(lambda score_dict: score_dict['compound'])\n",
    "df['neg']  = df['sentiment_dict'].apply(lambda score_dict: score_dict['neg'])*-1\n",
    "df['pos']  = df['sentiment_dict'].apply(lambda score_dict: score_dict['pos'])\n",
    "\n",
    "#  Prediction confidence\n",
    "df['confidence score'] = abs(df['neg']+df['pos'])/(abs(df['neg'])+abs(df['pos']))\n",
    "df['confidence score'] = df['confidence score'].replace(np.nan, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6fe81a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vader_sentiment_result(sent):\n",
    "    scores = analyzer.polarity_scores(sent)\n",
    "    if scores[\"compound\"] >=0.05: # positive, reference: https://pypi.org/project/vader-sentiment/\n",
    "        return 2\n",
    "    elif scores[\"compound\"] <=-0.05: # negative\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 # netural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "736375c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"vader_result\"] = df[\"body\"].apply(lambda x: vader_sentiment_result(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "367a946c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use very-high and high confidence groups of positive and negative predictions as training data\n",
    "df_train = df[df['confidence score']>0.5]\n",
    "df_test = df[~(df['confidence score']>0.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f228603d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.body.values\n",
    "y_train = df_train.vader_result.values\n",
    "X_test = df_test.body.values\n",
    "y_test = df_test.vader_result.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6049f770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to tokenize the input for encoder \n",
    "# Load bert tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased',do_lower_case=True)\n",
    "def preprocessing_for_bert(X, y, batch_size = 32):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    for sent in X:\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=sent,  \n",
    "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
    "            max_length = 512,                  # maximum twitter lenght 280\n",
    "            pad_to_max_length=True,         \n",
    "            #return_tensors='pt',           # Return PyTorch tensor\n",
    "            return_attention_mask=True,      \n",
    "            truncation=True\n",
    "            )\n",
    "        \n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "     \n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "    \n",
    "    labels = torch.tensor(y)\n",
    "    # convert the tensors into a PyTorch Dataset=\n",
    "    data = TensorDataset(input_ids, attention_masks, labels)\n",
    "    sampler = RandomSampler(data)\n",
    "    # feed dataset to training loop\n",
    "    dataloader = DataLoader(data, # The training samples.\n",
    "                            sampler=sampler,   # Select batches randomly\n",
    "                            batch_size=batch_size)\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cb45287d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader  = preprocessing_for_bert(X_train, y_train, batch_size = 32)\n",
    "test_dataloader  = preprocessing_for_bert(X_test, y_test, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "84ee5326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create the tensor dataset based on differnet confidence score\n",
    "# def subset(lower,higher):\n",
    "#     X = df[(df['confidence score']>lower)&(df['confidence score']<=higher)].tweet.values\n",
    "#     y = df[(df['confidence score']>lower)&(df['confidence score']<=higher)].vader_result.values\n",
    "#     return preprocessing_for_bert(X, y, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "dcabb4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vh_dataloader = subset(lower=0.75,higher=1)\n",
    "# h_dataloader = subset(lower=0.55,higher=0.75)\n",
    "# l_dataloader = subset(lower=0.38,higher=0.55)\n",
    "# vl_dataloader = subset(lower=0, higher=0.38)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f445fae",
   "metadata": {},
   "source": [
    "### 3. model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0dc3df06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(epochs=4):\n",
    "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
    "    \"\"\"\n",
    "    bert_classifier = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                      num_labels=3,\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)\n",
    "\n",
    "#     #Tell PyTorch to run the model on GPU\n",
    "#     bert_classifier.to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = AdamW(bert_classifier.parameters(),lr=1e-5) # change the learning rate to a lower number \n",
    "\n",
    "    # Total number of training steps\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    \n",
    "    # Warm up steps is a parameter which is used to lower the learning rate in order to reduce the impact of deviating the model from learning on sudden new data set exposure.\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=50, \n",
    "                                                num_training_steps=total_steps)\n",
    "    \n",
    "    return bert_classifier, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3ba5a42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_classifier, optimizer, scheduler = initialize_model(epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8c2d0361",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average='weighted')\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "        \n",
    "def kappa_score(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return cohen_kappa_scredfore(preds_flat,labels_flat, labels=None, weights=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a430e52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def other_metrics(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    f1_score = f1_score(labels_flat, preds_flat, average='weighted')\n",
    "    kappa_score = cohen_kappa_score(preds_flat,labels_flat, labels=None, weights=None)\n",
    "    print(\" F1 Score: {0:.2f}\".format(f1_score))\n",
    "    print(\" Kappa Score: {0:.2f}\".format(kappa_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6a9e7d",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "87e19cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "def train(model, train_dataloader, epochs):\n",
    "    \"\"\"\n",
    "    Train the BertClassifier model\n",
    "    \"\"\"\n",
    "    train_loss_set = []\n",
    "    print(\"Start training...\\n\")\n",
    "    \n",
    "    # We'll store a number of quantities such as training and validation loss, \n",
    "    # validation accuracy, and timings.\n",
    "    training_stats = []\n",
    "    \n",
    "    # Measure the total training time for the whole run.\n",
    "    total_t0 = time.time()\n",
    "    \n",
    "    # For each epoch...\n",
    "    for epoch_i in range(0, epochs):\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        print('Training...')\n",
    "        \n",
    "        t0 = time.time()  # Measure how long the training epoch takes.\n",
    "        total_train_loss = 0  # Reset the total loss for this epoch.\n",
    "        model.train()\n",
    "        \n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            if step % 40 == 0 and not step == 0:\n",
    "                # Calculate elapsed time in minutes.\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "            \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            \n",
    "            model.zero_grad()   \n",
    "            \n",
    "            output = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "            loss = output.loss\n",
    "            logits = output.logits\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            # Perform a backward pass to compute gradients\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            # Update the model’s parameters \n",
    "            optimizer.step()\n",
    "            # Update the learning rate\n",
    "            scheduler.step()\n",
    "            # Calculate the average loss over all of the batches.\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_dataloader) \n",
    "        training_time = format_time(time.time() - t0)\n",
    "        print(\"\")\n",
    "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "        print(\"  Training epcoh took: {:}\".format(training_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "63781348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    128.    Elapsed: 1:16:49.\n",
      "  Batch    80  of    128.    Elapsed: 2:32:55.\n",
      "  Batch   120  of    128.    Elapsed: 3:48:52.\n",
      "\n",
      "  Average training loss: 0.63\n",
      "  Training epcoh took: 4:03:16\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    128.    Elapsed: 1:15:10.\n",
      "  Batch    80  of    128.    Elapsed: 2:30:25.\n",
      "  Batch   120  of    128.    Elapsed: 3:45:54.\n",
      "\n",
      "  Average training loss: 0.30\n",
      "  Training epcoh took: 4:00:16\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    128.    Elapsed: 1:15:09.\n",
      "  Batch    80  of    128.    Elapsed: 2:30:25.\n",
      "  Batch   120  of    128.    Elapsed: 3:45:57.\n",
      "\n",
      "  Average training loss: 0.25\n",
      "  Training epcoh took: 4:00:32\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    128.    Elapsed: 1:23:01.\n",
      "  Batch    80  of    128.    Elapsed: 2:46:53.\n",
      "  Batch   120  of    128.    Elapsed: 4:04:28.\n",
      "\n",
      "  Average training loss: 0.25\n",
      "  Training epcoh took: 4:19:08\n"
     ]
    }
   ],
   "source": [
    "train(bert_classifier, train_dataloader, epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e7056a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model = './sentiment.pth'\n",
    "\n",
    "# save model\n",
    "def save(bert_classifier, optimizer):\n",
    "    # save\n",
    "    torch.save({\n",
    "        'bert_classifier_state_dict': bert_classifier.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, output_model)\n",
    "\n",
    "save(bert_classifier, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8e0f11f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to load\n",
    "# checkpoint = torch.load('./sentiment.pth')\n",
    "# bert_classifier.load_state_dict(checkpoint['bert_classifier_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fccf8a2",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "64145ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model,  prediction_dataloader):\n",
    "    # Put model in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Tracking variables \n",
    "    predictions , true_labels = [], []\n",
    "    \n",
    "    # Predict \n",
    "    for batch in prediction_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # Telling the model not to compute or store gradients, saving memory and speeding up predictio\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            # Forward pass, calculate logit predictions\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "        \n",
    "        logits = outputs[0]\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "    \n",
    "    flat_preds = [item for sublist in predictions for item in sublist]\n",
    "    probs = np.argmax(flat_preds, axis=1).flatten()\n",
    "    \n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f3591534",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = predict(bert_classifier,test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "67184bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_res = pd.DataFrame(probs, columns = ['Sentiment Score'])\n",
    "final_test = pd.concat([df_test.reset_index(drop=True), bert_res], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c88d64b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Sentiment Score'] = df_train['vader_result']\n",
    "final_table = pd.concat([final_test, df_train], axis=0)\n",
    "final_table.to_csv ('./bert_results.csv', index = True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9f9e8336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>created_at</th>\n",
       "      <th>n_words</th>\n",
       "      <th>sentiment_dict</th>\n",
       "      <th>compound</th>\n",
       "      <th>neg</th>\n",
       "      <th>pos</th>\n",
       "      <th>confidence score</th>\n",
       "      <th>vader_result</th>\n",
       "      <th>Sentiment Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>years bch narrative needed switch btc flippeni...</td>\n",
       "      <td>2022-02-05 01:32:11</td>\n",
       "      <td>10</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.891, 'pos': 0.109, 'comp...</td>\n",
       "      <td>0.0258</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.109</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>btc bch btc bch price matter</td>\n",
       "      <td>2022-02-04 22:43:39</td>\n",
       "      <td>6</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.82, 'pos': 0.18, 'compou...</td>\n",
       "      <td>0.0258</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.180</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>btc speculation sustainable seen btc price act...</td>\n",
       "      <td>2021-12-27 13:48:22</td>\n",
       "      <td>18</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.939, 'pos': 0.061, 'comp...</td>\n",
       "      <td>0.0258</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.061</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>need new ath btc pair usdt pair price matter much</td>\n",
       "      <td>2021-12-12 14:46:53</td>\n",
       "      <td>10</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.891, 'pos': 0.109, 'comp...</td>\n",
       "      <td>0.0258</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.109</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>article background information binance used te...</td>\n",
       "      <td>2021-11-15 23:05:46</td>\n",
       "      <td>17</td>\n",
       "      <td>{'neg': 0.063, 'neu': 0.937, 'pos': 0.0, 'comp...</td>\n",
       "      <td>-0.0191</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>bnb moves btc wallet exchange faster cheaper e...</td>\n",
       "      <td>2021-10-26 15:07:03</td>\n",
       "      <td>13</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.916, 'pos': 0.084, 'comp...</td>\n",
       "      <td>0.0258</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.084</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969</th>\n",
       "      <td>btc price matter anymore</td>\n",
       "      <td>2021-10-20 19:05:19</td>\n",
       "      <td>4</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.732, 'pos': 0.268, 'comp...</td>\n",
       "      <td>0.0258</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.268</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1281</th>\n",
       "      <td>reason tether gets printed unreasonable rates ...</td>\n",
       "      <td>2021-09-14 01:54:34</td>\n",
       "      <td>17</td>\n",
       "      <td>{'neg': 0.063, 'neu': 0.937, 'pos': 0.0, 'comp...</td>\n",
       "      <td>-0.0191</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1314</th>\n",
       "      <td>prices need eat based usd price btc fluctuate ...</td>\n",
       "      <td>2021-09-09 02:23:49</td>\n",
       "      <td>10</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.891, 'pos': 0.109, 'comp...</td>\n",
       "      <td>0.0258</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.109</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1612</th>\n",
       "      <td>let assume tether actually say 80 backed 40b b...</td>\n",
       "      <td>2021-07-27 15:13:49</td>\n",
       "      <td>14</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.922, 'pos': 0.078, 'comp...</td>\n",
       "      <td>0.0258</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.078</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3925</th>\n",
       "      <td>buy bunch btc tether printed backed anything r...</td>\n",
       "      <td>2021-01-11 10:52:47</td>\n",
       "      <td>22</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.95, 'pos': 0.05, 'compou...</td>\n",
       "      <td>0.0258</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4740</th>\n",
       "      <td>wait thought price matter bch btc</td>\n",
       "      <td>2020-11-20 02:24:13</td>\n",
       "      <td>6</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.82, 'pos': 0.18, 'compou...</td>\n",
       "      <td>0.0258</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.180</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5135</th>\n",
       "      <td>expansion possible price matter btc</td>\n",
       "      <td>2020-09-27 14:50:55</td>\n",
       "      <td>5</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.784, 'pos': 0.216, 'comp...</td>\n",
       "      <td>0.0258</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.216</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5863</th>\n",
       "      <td>btc able times run money point btc price matter</td>\n",
       "      <td>2020-06-05 03:45:29</td>\n",
       "      <td>9</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.879, 'pos': 0.121, 'comp...</td>\n",
       "      <td>0.0258</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.121</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6720</th>\n",
       "      <td>implying simplified btc bch needs reach market...</td>\n",
       "      <td>2020-03-12 14:37:58</td>\n",
       "      <td>13</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.916, 'pos': 0.084, 'comp...</td>\n",
       "      <td>0.0258</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.084</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6830</th>\n",
       "      <td>price matters btc bch back almost</td>\n",
       "      <td>2020-02-27 10:02:46</td>\n",
       "      <td>6</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.82, 'pos': 0.18, 'compou...</td>\n",
       "      <td>0.0258</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.180</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7575</th>\n",
       "      <td>mean cheap bch reach price btc</td>\n",
       "      <td>2019-12-11 09:20:34</td>\n",
       "      <td>6</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.82, 'pos': 0.18, 'compou...</td>\n",
       "      <td>0.0258</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.180</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9652</th>\n",
       "      <td>market illiquid tether supposed backed real do...</td>\n",
       "      <td>2019-06-21 07:12:49</td>\n",
       "      <td>18</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.939, 'pos': 0.061, 'comp...</td>\n",
       "      <td>0.0258</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.061</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10508</th>\n",
       "      <td>wonder price btc continue increasing fees reac...</td>\n",
       "      <td>2019-04-22 13:22:52</td>\n",
       "      <td>13</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.916, 'pos': 0.084, 'comp...</td>\n",
       "      <td>0.0258</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.084</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    body          created_at  \\\n",
       "96     years bch narrative needed switch btc flippeni... 2022-02-05 01:32:11   \n",
       "99                          btc bch btc bch price matter 2022-02-04 22:43:39   \n",
       "403    btc speculation sustainable seen btc price act... 2021-12-27 13:48:22   \n",
       "511    need new ath btc pair usdt pair price matter much 2021-12-12 14:46:53   \n",
       "677    article background information binance used te... 2021-11-15 23:05:46   \n",
       "915    bnb moves btc wallet exchange faster cheaper e... 2021-10-26 15:07:03   \n",
       "969                             btc price matter anymore 2021-10-20 19:05:19   \n",
       "1281   reason tether gets printed unreasonable rates ... 2021-09-14 01:54:34   \n",
       "1314   prices need eat based usd price btc fluctuate ... 2021-09-09 02:23:49   \n",
       "1612   let assume tether actually say 80 backed 40b b... 2021-07-27 15:13:49   \n",
       "3925   buy bunch btc tether printed backed anything r... 2021-01-11 10:52:47   \n",
       "4740                   wait thought price matter bch btc 2020-11-20 02:24:13   \n",
       "5135                 expansion possible price matter btc 2020-09-27 14:50:55   \n",
       "5863     btc able times run money point btc price matter 2020-06-05 03:45:29   \n",
       "6720   implying simplified btc bch needs reach market... 2020-03-12 14:37:58   \n",
       "6830                   price matters btc bch back almost 2020-02-27 10:02:46   \n",
       "7575                      mean cheap bch reach price btc 2019-12-11 09:20:34   \n",
       "9652   market illiquid tether supposed backed real do... 2019-06-21 07:12:49   \n",
       "10508  wonder price btc continue increasing fees reac... 2019-04-22 13:22:52   \n",
       "\n",
       "       n_words                                     sentiment_dict  compound  \\\n",
       "96          10  {'neg': 0.0, 'neu': 0.891, 'pos': 0.109, 'comp...    0.0258   \n",
       "99           6  {'neg': 0.0, 'neu': 0.82, 'pos': 0.18, 'compou...    0.0258   \n",
       "403         18  {'neg': 0.0, 'neu': 0.939, 'pos': 0.061, 'comp...    0.0258   \n",
       "511         10  {'neg': 0.0, 'neu': 0.891, 'pos': 0.109, 'comp...    0.0258   \n",
       "677         17  {'neg': 0.063, 'neu': 0.937, 'pos': 0.0, 'comp...   -0.0191   \n",
       "915         13  {'neg': 0.0, 'neu': 0.916, 'pos': 0.084, 'comp...    0.0258   \n",
       "969          4  {'neg': 0.0, 'neu': 0.732, 'pos': 0.268, 'comp...    0.0258   \n",
       "1281        17  {'neg': 0.063, 'neu': 0.937, 'pos': 0.0, 'comp...   -0.0191   \n",
       "1314        10  {'neg': 0.0, 'neu': 0.891, 'pos': 0.109, 'comp...    0.0258   \n",
       "1612        14  {'neg': 0.0, 'neu': 0.922, 'pos': 0.078, 'comp...    0.0258   \n",
       "3925        22  {'neg': 0.0, 'neu': 0.95, 'pos': 0.05, 'compou...    0.0258   \n",
       "4740         6  {'neg': 0.0, 'neu': 0.82, 'pos': 0.18, 'compou...    0.0258   \n",
       "5135         5  {'neg': 0.0, 'neu': 0.784, 'pos': 0.216, 'comp...    0.0258   \n",
       "5863         9  {'neg': 0.0, 'neu': 0.879, 'pos': 0.121, 'comp...    0.0258   \n",
       "6720        13  {'neg': 0.0, 'neu': 0.916, 'pos': 0.084, 'comp...    0.0258   \n",
       "6830         6  {'neg': 0.0, 'neu': 0.82, 'pos': 0.18, 'compou...    0.0258   \n",
       "7575         6  {'neg': 0.0, 'neu': 0.82, 'pos': 0.18, 'compou...    0.0258   \n",
       "9652        18  {'neg': 0.0, 'neu': 0.939, 'pos': 0.061, 'comp...    0.0258   \n",
       "10508       13  {'neg': 0.0, 'neu': 0.916, 'pos': 0.084, 'comp...    0.0258   \n",
       "\n",
       "         neg    pos  confidence score  vader_result  Sentiment Score  \n",
       "96    -0.000  0.109               1.0             1                1  \n",
       "99    -0.000  0.180               1.0             1                1  \n",
       "403   -0.000  0.061               1.0             1                1  \n",
       "511   -0.000  0.109               1.0             1                1  \n",
       "677   -0.063  0.000               1.0             1                1  \n",
       "915   -0.000  0.084               1.0             1                1  \n",
       "969   -0.000  0.268               1.0             1                1  \n",
       "1281  -0.063  0.000               1.0             1                1  \n",
       "1314  -0.000  0.109               1.0             1                1  \n",
       "1612  -0.000  0.078               1.0             1                1  \n",
       "3925  -0.000  0.050               1.0             1                1  \n",
       "4740  -0.000  0.180               1.0             1                1  \n",
       "5135  -0.000  0.216               1.0             1                1  \n",
       "5863  -0.000  0.121               1.0             1                1  \n",
       "6720  -0.000  0.084               1.0             1                1  \n",
       "6830  -0.000  0.180               1.0             1                1  \n",
       "7575  -0.000  0.180               1.0             1                1  \n",
       "9652  -0.000  0.061               1.0             1                1  \n",
       "10508 -0.000  0.084               1.0             1                1  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_table[final_table['Sentiment Score']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63888159",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}